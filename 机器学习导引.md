# 机器学习导引

## 名词解释

### 第一章序和绪论

1. 机器学习

   从数学角度机器学习就是从数据中学习到一个函数f

   利用数据自我进化的工具

   从经验数据中提升性能

   数据挖掘：从数据中发现模式的过程

2. 模式识别：模式识别是机器学习中的一个分支主要关注数据中的模式和规律的识别

3. 模式：是指具体识别问题中的具有像同类别属性的事物或行为的统称

4. 数据预处理：保留和恢复与识别相关的，去除不相关的部分

5. 特征提取：就是将数据换种方式表达，特征提取将原始观测数据变换到新的原始空间，是的学习和识别更容易，效果更好

6. 有监督学习：

   监督学习是指数据集的正确输出已知情况下的一类学习算法。因为输入和输出已知，意味着输入和输出之间有一个关系，监督学习算法就是要发现和总结这种“关系”。

   *简单的归纳就是，是否有监督（supervised），就看输入数据是否有标签（label）。输入数据有标签，则为有监督学习；没标签则为无监督学习*

7. 无监督学习：无监督学习是指对无标签数据的一类学习算法。因为没有标签信息，意味着需要从数据集中发现和总结模式或者结构。

### 第二章模型的评估与选择

1. 经验误差（Empirical Error）：学习器f在训练集上的误差
2. 泛化误差（Generalization Error）：学习器在未来样本上的误差
3. 欠拟合：相较于数据而言，模型参数过少或者模型结构过于简单以至于无法捕捉中的规律的现象
4. 过拟合：模型过于紧密或精确地匹配特定数据集，以至于无法良好的拟合其他数据或预测未来的观察结果的现象
5. 合适的拟合：模型能够恰当地拟合和捕捉数据中的规律和现象
6. 留出法：直接将数据集D划分为两个互斥的集合，一个作为训练集S，另一个作为测试集 T，即 D=S∪T，S∩T=∅。
7. 交叉验证法：随机将样本拆分成K个互不相交大小相同的子集，然后用K-1个子集作为训练集训练模型，用剩余的子集测试模型，对K中选择重复进行，最终选出K次测评中的平均测试误差最小的模型。
8. 自助法（bootstrapping，有放回采样）：留出法每次从数据集 D 中抽取一个样本加入数据集 D′ 中，然后再将该样本放回到原数据集 D 中，即 D 中的样本可以被重复抽取。
9. 调参
10. **偏差（Bias）**: 指的是模型在多次训练过程中预测结果的平均值与真实值之间的差异。偏差较高通常意味着模型过于简单，不能捕捉数据的真实规律，即所谓的“欠拟合”（underfitting）。
11. **方差（Variance）**: 描述的是模型对于给定的数据点，预测结果随着不同训练数据的变化而波动的程度。高方差通常意味着模型过于复杂，对训练数据中的随机噪声非常敏感，即所谓的“过拟合”（overfitting）。
12. **误差（Error）**: 总误差可以分解为偏差的平方、方差，以及不可避免的误差ε²。这个不可避免的误差是由于数据本身的随机性或者噪声导致的，是模型无法减少的误差部分。

### 第三章线性模型

1. MSE均方误差
2. 最小二乘法（Last square method）：基于均方误差最小化求解模型参数的方法
3. 概率（Probility）：参数已知求实验结果的可能性
4. 似然(likelihood)：实验结果已知求参数的可能性
5. 极大似然法（Maximum likelihood）:寻求参数使得已发生的实验结果的可能性最大
6. 梯度下降法(Grandient Descent)：是一种一阶优化办法，用来求无约束优化问题。步骤先设定一个初始点然后，然后求该点的梯度，然后根据梯度和学习率求出x的变化量，确保$f(x_t)>f(x_{t+1})$，直到求得极小值。
7. 线性判别分析（Linear Discriminate Analysis,LDA）的思想：寻找一个直线（或者低维子空间），使得同类的样本尽可能的接近，异类的样本尽可能的远离

### 第四章决策树(Decision Tree)

1. 决策树的策略：分而治之，自根至叶递归的过程 

2. 信息量：$I(x)=-log_{2}p(x)$
3. 信息熵：平均信息量$H(x)=\sum_{i=1}^{n}I(x_i)p(x_i)$
4. 预剪枝：训练时间开销少，测试时间开销降低，过拟合风险减低，欠拟合风险**增加**
5. 后剪枝：训练时间开销**增加**，测试时间开销降低，过拟合风险减低，欠拟合风险**基本不变**

### 第五章神经网络

1. 模式分类问题的目标为:设计智能算法自动将新的测试样本准确划分到真实的类别。
2. 感知器的过程：
   1. 计算感知器的估计结果
   2. 更新权重向量
   3. 重复以上步骤直到对于所有训练样本没有分类错误或者达到一定的迭代次数
3. 批次梯度下降（batch Gradient Descent，BGD)$w_{t+1}=w_t-\eta \frac{1}{n}\sum^n_{i=1}\bigtriangledown e_i(w_t)$
4. 随机梯度下降$w_{t+1}=w_t-\eta \bigtriangledown e_i(w_t)$
5. 小批量梯度下降$w_{t+1}=w_t-\eta \frac{1}{k}\sum_{i\in B_k} \bigtriangledown e_i(w_t)$
6. 为什么要卷积：稀疏连接
7. 卷积神经网络CNN：
8. 数据增强
9. dropout 

### 第六章支持向量机